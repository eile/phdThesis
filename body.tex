\pagenumbering{arabic}

\chapter{Background}

\section{Motivation}

After decades of exponential growth in computational performance, storage and
data acquisition, computing is now well in the big data age, where future
advances are measured in our capability to extract meaningful information from
the available data. Visual analysis based on interactive rendering of
three-dimensional data has been proven to be a particularly efficient approach
to gain intuitive insight into the spatial structure and relations of very large
3D data sets. These developments create new, unique challenges for applications
and system software to enable users to fully exploit the available resources to
gain insight from their data.

The quantity of computed, measured or collected data is exponentially growing,
fueled by the pervasive diffusion of digitalization in modern life. Moreover,
the fields of science, engineering and technology are increasingly defined by a
data driven approach to conduct research and development. High-quality and
large-scale data is continuously generated at a growing rate from sensor and
scanning systems, as well as from data collections and numerical simulations in
a number of science and technology domains.

\begin{figure}[ht]\label{FIG_teaser}
\includegraphics[height=5cm]{images/slices}\hfil%
\includegraphics[height=5cm]{images/cave2}\\%
\includegraphics[height=5.27cm]{images/cave}\hfil%
\includegraphics[height=5.27cm]{images/tide}%
\caption{Large Data Visualization: Large data visualization of a
      brain simulation, molecular visualization in the Cave$^2$, exploration of EM
      stack reconstructions in a Cave, collaborative data analysis on a tiled
      display wall.}
\end{figure}

Display technology has made significant progress in the last decade.
High-resolution screens and tiled display walls are now affordabe for most
organizations and are getting deployed at an increasing rate. This increased
resolution and display size helps with understanding the data, but with the
quadratic increase in pixels to be rendered, it increases the pressure on
rendering algorithms to deliver interactive framerates. Furthermore, for larger
system it becomes necessary to develop parallel and distributed applications.

However, not only applications are becoming more and more data-driven, but also
the technology used to tackle these kinds of problems is rapidly witnessing a
paradigm shift towards massively parallel on-chip and distributed parallel
cluster solutions. On one hand, parallelism within a system has increased
massively, with tenths of CPU cores, thousands of GPU cores and multiple CPUs
and GPUs in a single system. On the other hand, massively parallel distributed
systems are easily accessible from various cloud infrastructure providers, and
are also affordable for on-site hosting for many organizations.

System software to exploit the available hardware parallelism capable of
performing efficient interactive data exploration has not kept up with the pace
in hardware developments and data gathering capabilities. On one hand, this is
due to an inherent delay between hardware and software capabilities, since
development typically only starts once the hardware is available. On the other
hand, existing software engineered for different design parameters has a
significant inertia to change, to the extreme of the necessity to rewrite it
from scratch.

In the context of emerging data-intensive knowledge discovery and data analysis,
efficient interactive data exploration methodologies have become increasingly
important. Visual analysis by means of interactive visualization and inspection
of three-dimensional data is a particularly efficient approach to gain intuitive
insight into the spatial structure and relations of very large 3D data sets.
However, defining visual and interactive methods scalable with problem size and
degree of parallelism, as well as generic applicability of high-performance
interactive visualization methods and systems are recognized among the major
current and future challenges.

\section{Interactive Visualization}

\section{Parallel Rendering}

The main performance indicator for Large Data Interactive Rendering is the
performance of the rendering algorithm, that is, the framerate with which the
program produces new images. This framerate can be improved by either using
faster or more hardware, or by better algorithms exploiting the existing
hardware and data. This proposal primarily focuses on the first approach using
parallel rendering to exploit the CPU and GPU parallelism available on a single
system or a distributed cluster.  The early fundamental concepts have been laid
down in \cite{MCEF:94} and \cite{Crockett:97}. A number of domain specific
parallel rendering algorithms and special-purpose hardware solutions have been
proposed in the past, however, only few generic parallel rendering frameworks
have been developed (\fig{fSorts}). We will focus on sort-last and sort-first
rendering, since sort-middle architectures are only feasible in a hardware
implementation due to the large amount of fragments processed and transferred in
the sorting stage.

\begin{figure}[ht]\center
\includegraphics[width=0.7\textwidth]{images/all_sorts}%
\caption{Sort-last, sort-middle and sort-first parallel rendering\label{fSorts}}
\end{figure}


\subsection{Domain specific solutions}

Cluster-based parallel rendering has been commercialized for off-line rendering
(i.e. distributed ray-tracing) for computer generated animated movies or special
effects, since the ray-tracing technique is inherently amenable to
parallelization for off-line processing. Other special-purpose solutions exist
for parallel rendering in specific application domains such as volume rendering
\cite{LWMT:97,Wittenbrink:98,HSCSM:00,SL:02,GS:02,NSJLYZ:05} or
geo-visualization \cite{VR:91,AG:95,LDC:96,JLMV:06}. However, such specific
solutions are typically not applicable as a generic parallel rendering paradigm
and do not translate to arbitrary scientific visualization and distributed
graphics problems.

In \cite{NC:07}, parallel rendering of hierarchical level-of-detail (LOD) data
has been addressed and a solution specific to sort-first tile-based parallel
rendering has been presented. While the presented approach is not a generic
parallel rendering system, basic concepts presented in \cite{NC:07} such as load
management and adaptive LOD data traversal can be carried over to other
sort-first parallel rendering solutions.

\subsection{Special-purpose architectures}

Historically, high-performance real-time rendering systems have relied on an
integrated proprietary system architecture, such as the early SGI graphics super
computers. These special-purpose solutions have become a niche product as their
graphics performance does not keep up with off-the-shelf workstation graphics
hardware and scalability of clusters.

Due to its conceptual simplicity, a number of special-purpose image compositing
hardware solutions for sort-last parallel rendering have been developed. The
proposed hardware architectures include Sepia \cite {MHS:99a,sepia}, Sepia~2
\cite{LMSBHa:01,LMSBH:01}, Lightning~2 \cite{Stoll01}, Metabuffer
\cite{Blanke00,Zhang01}, MPC Compositor \cite{Muraki01} and PixelFlow
\cite{Molnar92, Eyles97}, of which only a few have reached the commercial
product stage (i.e. Sepia~2 and MPC Compositor). However, the inherent
inflexibility and setup overhead have limited their distribution and application
support. Moreover, with the recent advances in the speed of CPU-GPU interfaces,
such as PCI Express, NVLink and other modern interconnects, combinations of
software and GPU-based solutions offer more flexibility at comparable
performance.

\subsection{Generic approaches}

A number of algorithms and systems for parallel rendering have been developed in
the past. On one hand, some general concepts applicable to cluster parallel
rendering have been presented in \cite{Mueller:95,Mueller:97} (sort-first
architecture), \cite{SZFLS:99,SFLS:00} (load balancing), \cite{SFL:01} (data
replication), or \cite{CMF:05,CM:06} (scalability). On the other hand, specific
algorithms have been developed for cluster based rendering and compositing such
as \cite{AP:98}, \cite{CKS:02} and \cite{YYC:01,SMLAP:03}. However, these
approaches do not constitute APIs and libraries that can readily be integrated
into existing visualization applications, although the issue of the design of a
parallel graphics interface has been addressed in \cite{Igehy98}.

Only few generic APIs and (cluster-)parallel rendering systems exist which
include VR Juggler \cite{BJHMBC:01} (and its derivatives), Chromium
\cite{HHNFAKK:02} (an evolution of \cite{Humphreys99,Humphreys00,HEBSEH:01}),
{ClusterGL}~\cite{NHM:11} and OpenGL Multipipe SDK
\cite{JDBJBCER:04,BRE:05,MPK}. These approaches can be categorized into
transparent interception and distribution of the OpenGL command stream and into
the parallization of the application rendering code (\fig{fChromium}).

\begin{figure}[ht]
\includegraphics[height=4cm]{images/Chromium}\hfil%
\includegraphics[height=4cm]{images/MPK}%
\caption{Transparent OpenGL interception and parallelization of the rendering code\label{fChromium}}
\end{figure}


VR Juggler \cite{BJHMBC:01,JBBC:98} is a graphics framework for virtual reality
applications which shields the application developer from the underlying
hardware architecture, devices and operating system. Its main aim is to make
virtual reality configurations easy to set up and use without the need to know
details about the devices and hardware configuration, but not specifically to
provide scalable parallel rendering. Extensions of VR Juggler, such as for
example ClusterJuggler \cite{BC:03} and NetJuggler \cite{AGLMR:02}, are
typically based on the replication of application and data on each cluster node
and basically take care of synchronization issues, but fail to provide a
flexible and powerful configuration mechanism that efficiently supports scalable
rendering as also noted in \cite{SWNH:03}. VR Juggler does not support scalable
parallel rendering such as sort-first and sort-last task decomposition and image
compositing nor does it provide support for network swap barriers
(synchronization), distributed objects, image compression and transmission, or
multiple rendering threads per process, important for multi-GPU systems.

While Chromium \cite{HHNFAKK:02} provides a powerful and transparent abstraction
of the OpenGL API, that allows a flexible configuration of display resources,
its main limitation with respect to scalable rendering is that it is focused on
streaming OpenGL commands through a network of nodes, often initiated from a
single source. This has also been observed in \cite{SWNH:03}. The problem comes
in when the OpenGL stream is large in size, due to not only containing OpenGL
calls but also the rendered data such as geometry and image data. Only if the
geometry and textures are mostly static and can be kept in GPU memory on the
graphics card, no significant bottleneck can be expected as then the OpenGL
stream is composed of a relatively small number of rendering instructions.
However, as it is typical in real-world visualization applications, display and
object settings are interactively manipulated, data and parameters may change
dynamically, and large data sets do not fit statically in GPU memory but are
often dynamically loaded from out-of-core and/or multiresolution data
structures. This can lead to frequent updates not only of commands and
parameters wich have to be distributed but also of the rendered data itself
(geometry and texture), thus causing the OpenGL stream to expand dramatically.
Furthermore, this stream of function calls and data must be packaged and
broadcast in real-time over the network to multiple nodes for each rendered
frame. This makes CPU performance and network bandwidth a more likely limiting
factor.

The performance experiments in \cite{HHNFAKK:02} indicate that Chromium is
working quite well when the rendering problem is fill-rate limited. This is due
to the fact that the OpenGL commands and a non-critical amount of rendering data
can be distributed to multiple nodes without significant problems and since the
critical fill-rate work is then performed locally on the graphics hardware.

Chromium also provides some facilities for parallel application development,
namely a sort-last, binary-swap compositing SPU and an OpenGL extension
providing synchronization primitives, such as a barrier and semaphore. It leaves
other problems, such as configuration, task decomposition as well as process and
thread management unaddressed. Parallel Chromium applications tend to be written
for one specific parallel rendering use case, such as for example the sort-first
distributed memory volume renderer \cite{BHPB:03} or the sort-last parallel
volume renderer raptor \cite{Raptor}. We are not aware of a generic
Chromium-based application using many-to-one sort-first or stereo
decompositions.

The concept of transparent OpenGL interception popularized by WireGL and
Chromium has received some further contributions. While some commercial
implementations such as {TechViz} and {MechDyne Conduit} continue to exist, on
the research side only {ClusterGL}~\cite{NHM:11} has been presented recently.
{ClusterGL} employs the same approach as {Chromium}, but delivers a
significantly faster implementation of transparent OpenGL interception and
distribution for parallel rendering. Transparent OpenGL interception is an
appealing apporach for some applications since it requires no code changes, but
it has inherent limitiations due to the fact that eventually the bottleneck
becomes the single-threaded application rendering code, the amount of
application data the single application instance can load or process, or the the
size of the OpenGL command stream send over the network.

{CGLX}~\cite{DK:11} tries to bring parallel execution transparently to
OpenGL applications, by emulating the GLUT API and intercepting certain OpenGL
calls. In contrast to frameworks like {Chromium} and {ClusterGL}
which distribute OpenGL calls, {CGLX} follows the distributed application
approach. This works transparently for trivial applications, but quickly
requires the application developer to address the complexities of a distributed
application, when mutable application state needs to be synchronized across
processes. For realistic applications, writing parallel applications remains the
only viable approach for scalable parallel rendering, as shown by the success of
{Paraview}, {Visit} and {Equalizer}-based applications.

OpenGL Multipipe SDK (MPK) \cite{BRE:05} implemented an effective parallel
rendering API for a shared memory multi-CPU/GPU system. It is similar to IRIS
Performer \cite{RH:94} in that it handles multi-GPU rendering by a lean
abstraction layer via a conceptual callback mechanism, and that it runs
different application tasks in parallel. However, MPK is not designed nor meant
for rendering nodes separated by a network. MPK focuses on providing a parallel
rendering framework for a single application, parts of which are run in parallel
on multiple rendering channels, such as the culling, rendering and final image
compositing processes.

Software for driving and interacting with tiled display walls has received
significant attention, including {Sage}~\cite{Sage} and
{Sage~2}~\cite{Sage2} in particular. {Sage} was built entirely
around the concept of a shared framebuffer where all content windows are
separate applications using pixel streaming but is no longer actively supported.
{Sage 2} is a complete, browser-centric reimplementation where each
application is a web application distributed across browser instances.
{DisplayCluster}~\cite{DisplayCluster}, and its continuation
{Tide}~\cite{tide}, also implement the shared framebuffer concept of
{Sage}, but provide a few native content applications integrated into the
display servers. These solutions implement a scalable display environment and
are a target display platform for scalable 3D graphics applications.

\section{Dissertation Structure}


\chapter{Contributions}

\section{Generic Parallel Rendering Framework}

\section{Parallel Rendering Modes}

\section{Load Balancing}

\section{Applications}


\chapter{Parallel Rendering Framework Architecture}

\section{Overview}
A generic parallel rendering framework has to cover a wide range of use cases,
target systems, and configurations. This requires on one hand a strong
separation between the implementation of an application and its configuration,
and on the other hand a careful design to allow the resulting program to scale
up to hundreds of nodes, while providing a minimally invasive API for the
developer. In this section we present the system architecture of the Equalizer
parallel rendering framework, and motivate its design in contrast to related
work.

The motivation to use parallel rendering is either driven by the need to drive
multiple displays or projectors from multiple GPUs and potentially multiple
nodes, or by the need to increase rendering performance to be able to visualize
more data or use a more demanding rendering algorithm for higher visual quality.
Occasionally both needs coincide, for example for the analysis for large data
sets on high fidelity visualization systems.

Fundamentally, there are two approaches to enable applications to use multiple
GPUs: transparent interception at the graphics API (typically OpenGL) or
extending the application to support parallel rendering natively. The first
approach has been extensively explored by Chromium and others, while the second
is the foundation for this thesis. The architecture of Equalizer is founded on
an in-depth requirements analysis of typical visualization applications,
existing frameworks, and previous work on OpenGL Multipipe SDK.

The task of parallelizing a visualization application boils down to configuring
the application's rendering code differently for each resource, enabling this
rendering code to access the correct data, and synchronizing execution. For
scalable rendering, when multiple GPUs are used to accelerate a single output,
partial results need to be collected from all contributing resources and
combined on the output. Equalizer has a strong seperation of the rendering code
from its runtime configuration. The configuration is layed out in a hierarchical
resource description and compound trees configuring the resources for parallel
and scalable rendering. The configuration is a dynamic runtime structure, with a
serialized configuration file format.

In the following we will first describe the execution model and runtime
configurability, followed by the how the generic configuration is used to model
the desired visualization setup, and finally introduce specifics of scalable and
distributed rendering.

\section{Asynchronous Execution Model}

The core execution model for parallel rendering was pioneered by CAVELib
\cite{DACNCCGHPSNS:97}, refined by OpenGL Multipipe SDK for shared memory
systems and scalable rendering, and substantially extended by Equalizer for
asynchronous and distributed execution. By analysing the typical architecture of
a visualization application we observe an initialization phase, a main rendering
loop, and an exit phase. Equalizer decomposes these steps for parallel
execution.

The main rendering loop typically consists of four phases: submitting the
rendering commands to the graphics subsystem, displaying the renderered image,
and retrieving events from the operating system, based on which the application
state is updated and a new image is rendered. The configuration of the rendering
is largely hard-coded, with a few configurables such as field of view or stereo
separation. For parallel execution, we need to separate the rendering code from
this main loop, and execute it in parallel with different rendering parameters,
as shown in \fig{FIG_execution}. Similarly, the initialization and exit phase
also needs to be decomposed to allow managing multiple, distributed resources.

\begin{figure}[ht]\center
  \includegraphics[width=.9\columnwidth]{images/executionFlow}
  \caption{Simplified execution flow of a classical visualization application
    and an Equalizer application.}
  \label{FIG_execution}
\end{figure}

Without going into the details at this point, another critical design parameter
are synchronization points. Most implementations use a per-frame barrier or
similar synchronization to manages parallel execution. In larger installations,
this is detrimental for scalability, as even slight load imbalances limit
parallel speedup. The Equalizer execution model is fully asynchronuous, and only
introduces synchronization points when strictly needed. The main synchronization
points are: configured swap barriers between a set of output which have to
display simultaneously, the availability of input frames for scalable rendering,
and a task synchronization to prevent runaway of the main loop execution. By
default, Equalizer keeps up to one frame of latency in execution, that is, some
resource might render the next frame while others are still finishing the
current. Nonetheless, resources which are ready will immediately display their
result. The asynchronous execution architecture, coupled with a frame of
latency, allows pipelining of many operations, such as the application event
processing, task computation and load balancing, rendering, image readback,
compression, network transmission, and compositing.

\fig{fSyncAsync} shows the execution of the rendering tasks of a 2-node
sort-first compound without latency and with a latency of one frame. The
asynchronous execution pipelines rendering operations and hides imbalances in
the load distribution, resulting in an improved framerate. For example, we have
observed a speedup of 15\% on a five-node rendering cluster when using a latency
of one frame instead of no latency.

\begin{figure}[ht!]\center
  \includegraphics[width=\textwidth]{images/syncAsync}
  {\caption{\label{fSyncAsync}Synchronous and Asynchronous Execution}}
\end{figure}


\subsection{Programming Interface}

Equalizer provides a framework to facilitate the development of distributed as
well as non-distributed parallel rendering applications. The programming
interface is based on a set of C++ classes, modeled closely to the resource
hierarchy of a graphics rendering system. The application subclasses these
objects and overrides C++ task methods, similar to C callbacks. These task
methods will be called in parallel by the framework, depending on the current
configuration. This parallel rendering interface is significantly different from
Chromium \cite{HHNFAKK:02} and more similar to VRJuggler \cite{BJHMBC:01} or
OpenGL Multipipe SDK \cite{BRE:05}.

\begin{wrapfigure}{r}{.618\textwidth}
  \includegraphics[width=.618\textwidth]{images/processes}
  {\caption{\label{fProcessing}Parallel Rendering Entities}}
\end{wrapfigure}

To separate the responsibilities in a parallel rendering application, different
entities are resonsible for different aspects of the runtime system: the
application process to drive a rendering session, the server process or thread
to control the parallel rendering configuration, render clients to execute the
rendering tasks, and an administrative API to reconfigure the rendering session
at runtime.

\subsection{Application}

The main application thread in Equalizer drives the rendering, that is, it
carries out the main rendering loop, but does not actually execute any
rendering. Depending on the configuration, the application process often hosts
one or more render client threads. When a configuration has no additional nodes
besides the application node, all application code is executed in the same
process, and no network data distribution has to be implemented. The main
rendering loop is quite simple: The application requests a new frame to be
rendered, synchronizes on the completion of a frame and processes events
received from the render clients. Figure~\ref{FIG_execution} shows a simplified
execution model of an Equalizer application.


\subsection{Server}

The Equalizer server manages the parallel rendering session. It is an
asynchronous execution thread or process which receives requests from the
application and serves these requests using the current configuration, launching
and stopping rendering client processes on nodes, determining the rendering
tasks for a frame, and synchronizing the completion of tasks.

\subsection{Render Client}

During initialization of the server, the application provides a rendering client
executable. The rendering client is often, especially for simple applications,
the same executable as the application. However, in more sophisticated
implementations the rendering client can be a smaller executable which only
contains the application-specific rendering code. The server deploys this
rendering client on all nodes specified in the configuration.

In contrast to the application process, the rendering client does not have a
main loop and is completely controlled by the Equalizer framework based on
application's commands. A render client consists of the following threads: the
node main thread, one network receive thread, and one thread for each graphics
card (GPU) to execute rendering tasks. If a configuration also uses the
application node for rendering, then the application process uses one or more
render threads, consistent with render client processes.

The Equalizer client library implements the main loop, which receives network
events, processes them, and invokes the necessary task methods provided by the
developer.

The task methods clear the frame buffer as necessary, execute the OpenGL
rendering commands as well as readback, and assemble partial frame results for
scalable rendering. All tasks have default implementations so that only the
application specific methods have to be implemented, which at least involves the
{\tt frameDraw()} method executing a rendering task. For example, the default
callbacks for frame recomposition during scalable rendering implement tile-based
assembly for sort-first and stereo decompositions, and $z$-buffer compositing
for sort-last rendering.

\subsubsection{Render Context}

The render context is the core entity abstracting the application-specific
rendering algorithm from the system-specific configuration. It specifies:

\begin{compactdesc}
\item[Buffer] OpenGL-style read and draw buffer as well as color mask.
  These parameters are influenced by the current eye pass, eye
  separation and anaglyphic stereo settings.
\item[Viewport] Two-dimensional pixel viewport restricting the
  rendering area. For correct operations, both
  \textsf{glViewport} and \textsf{glScissor} have to be used. The pixel
  viewport is influenced by the destination viewport
  definition and viewports set for sort-first/2D decompositions.
\item[Frustum] Frustum parameters as defined by
  \textsf{glFrustum}. Typically the frustum used to set up the OpenGL projection
  matrix. The frustum is influenced by the destination's view
  definition, sort-first decomposition, tracking head matrix and the current eye pass.
\item[Head Transformation] A transformation matrix positioning the frustum. For
  planar views this is an identity matrix and is used in immersive rendering.
  It is normally used to set up the `view' part of the modelview matrix, before
  static light sources are defined.
\item[Range] A one-dimensional range with the interval [0..1]. This parameter is
  optional and should be used by the application to render only the appropriate
  subset of its data for sort-last rendering.
\end{compactdesc}

\subsubsection{Event Handling}

Event handling routes events from the source (the window in the rendering
thread) gradually to the the application main thread for consumption. At each
step, events can be observed, transformed or dropped. Events are received from
the operating system in the rendering thread, transformed there into a generic
representation, and sent over the network to the application. The application
processes them in the main loop and modifies its internal state accordingly.


\section{Configuration}

A configuration consists of the declaration of the rendering (hardware)
resources, the physical and logical  description of the projection system, and
the description on how the  aforementioned resources are used for parallel and
scalable rendering.

The rendering resources are represented in a hierarchical tree structure
which corresponds to the physical and logical resources found in a 3D
rendering environment: nodes (computers), pipes (graphics cards),
windows, and channels.

Physical layouts of display systems are configured using canvases with
segments, which represent 2D rendering areas composed of multiple
displays or projectors. Logical layouts are applied to canvases and
define views on a canvas.

Scalable resource usage is configured using a compound tree, which is a
hierarchical representation of the rendering decomposition and
recomposition across the resources.

\subsection{Rendering Resources}

The first part of the configuration is a hierarchical structure of
nodes-pipes-win\-dows-channels describing the rendering resources. The developer
will use instances of these classes to implement application logic and manage
data.

The \textsf{node} is the representation of a single computer in a cluster. One operating
system process of the render client executable will be used for each node. Each
configuration might also use an application node, in which case the application
process is also used for rendering. All node-specific task methods are executed
from the main thread.

The \textsf{pipe} is the abstraction of a graphics card (GPU), and uses an
operating system thread for rendering. All pipe, window and channel task methods
are executed from the pipe thread. The pipe maintains the information about the
GPU to be used by the windows for rendering.

The \textsf{window} encapsulates a drawable and an OpenGL context. The drawable
can be an on-screen window or an off-screen pbuffer or framebuffer object (FBO).
Windows on the same pipe share their OpenGL rendering resources. They execute
their rendering tasks sequentially on the pipe's execution thread.

The \textsf{channel} is the abstraction of an OpenGL viewport within its parent
window. It is the entity executing the actual rendering. The channel's viewport
is overwritten when it is rendering for another channel during scalable
rendering. Multiple channels in application windows may be used to view the
model from different viewports. Sometimes, a single window is split across
multiple projectors, e.g., by using an external splitter such as the Matrox
TripleHead2Go.


\subsection{Display Resources}

Display resources are the second part of a configuration. They describe the
physical display setup (canvases-segments), logical display (layouts-views) and
head tracking of users within the visualization installation (observers).

A \textsf{canvas} represents one physical projection surface, e.g., a PowerWall,
a curved screen or an immersive installation. Canvases provide a convenient way
to configure projection surfaces. A canvas uses layouts, which describe logical
views. Typically, each desktop window uses one canvas, one segment, one layout
and one view. One configuration might drive multiple canvases, for example an
immersive installation and an operator station. Planar surfaces, e.g., a display
wall, configure a frustum for the respective canvas. For non-planar surfaces,
the frustum will be configured on each display segment.

\begin{wrapfigure}{r}{.618\textwidth}
  \includegraphics[width=.618\textwidth]{images/frusta.pdf}
  {\caption{\label{fFrusta}Wall and Projection Parameters}}
\end{wrapfigure}

The frustum can be specified as a wall or projection description in the same
reference system as used by the head-tracking matrix calculated by the
application.  A wall is completely defined by the bottom-left, bottom-right and
top-left coordinates relative to the origin.  A projection is defined by the
position and head-pitch-roll orientation of the projector, as well as the
horizontal and vertical field-of-view and distance of the projection wall.
\fig{fFrusta} illustrates the wall and projection frustum parameters.

A canvas consists of one or more segments. A planar canvas typically has a
frustum description (see \sref{sFrustum}), which is inherited by the segments.
Non-planar frusta are configured using the segment frusta. These frusta
typically describe a physically correct display setup for Virtual Reality
installations.

A canvas has one or more layouts. One of the layouts is the active
layout, that is, this set of views is currently used for rendering. It
is possible to specify \textsf{OFF} as a layout, which deactivates the
canvas. It is possible to use the same layout on different canvases.

\begin{wrapfigure}{r}{.382\textwidth}
  \includegraphics[width=.382\textwidth]{images/canvas.pdf}
  {\caption{\label{fCanvas}A Canvas using four Segments}}
\end{wrapfigure}

A \textsf{segment} represents one output channel of the canvas, e.g., a
projector or a display. A segment has an output channel, which references the
channel to which the display device is connected. To synchronize the video
output, a canvas swap barrier or a swap barrier on each segment synchronize the
respective window buffer swaps.

A segment covers a part of its parent canvas, which is configured using the
segment viewport. The viewport is in normalized coordinates relative to the
canvas. Segments might overlap (edge-blended projectors) or have gaps between
each other (display walls, \fig{fCanvas}\footnote{Dataset courtesy of VolVis
  distribution of SUNY Stony Brook, NY, USA.}). The viewport is used to
configure the segment's default frustum from the canvas frustum description, and
to place layout views correctly.

A \textsf{layout} is the grouping of logical views. It is used by one or more
canvases. For all given layout/canvas combinations, Equalizer creates
destination channels when the configuration file is loaded. These destination
channels are later referenced by compounds to configure scalable rendering.
Layouts can be switched at runtime by the application. Switching a layout will
activate different destination channels for rendering.


\begin{wrapfigure}{r}{.618\textwidth}
  \includegraphics[width=.618\textwidth]{images/layout.png}
  {\caption{\label{fLayout}Layout with four Views}}
\end{wrapfigure}

A \textsf{view} is a logical view of the application data, in the sense used by
the Model-View-Controller pattern. It can be a scene, viewing mode, viewing
position, or any other representation of the application's data. A view has a
fractional viewport relative to its layout. A layout is often fully covered by
its views, but this is not a requirement. Each view can have a frustum
description. The view's frustum overrides frusta specified at the canvas or
segment level. This is typically used for non-physically correct rendering,
e.g., to compare two models side-by-side on a canvas. If the view does not
specify a frustum, it will use the sub-frustum resulting from the covered area
on the canvas.

A view might have an observer, in which case its frustum is tracked by this
observer. \fig{fLayout} shows an example layout using four views on a single
segment. \fig{fDisplay} shows a real-world setup of a single canvas with six
segments using underlap, with a two-view layout activated. This configuration
generates eight destination channels.

\begin{figure}[ht!]\center
  \includegraphics[width=.9\textwidth]{images/wallLayout.jpg}
  {\label{fDisplay}\caption{Display Wall using a six-Segment Canvas with a two-View Layout}}
\end{figure}

An \textsf{observer} represents an actor looking at multiple views. It has a
head matrix, defining its position and orientation within the world, an eye
separation and focus distance parameters. Typically, a configuration has one
observer. Configurations with multiple observers are used if multiple,
head-tracked users are in the same configuration session, e.g., a non-tracked
control host with two tracked head-mounted displays.

\subsection{Compounds}

Compound trees are used to describe how multiple rendering resources are
combined to produce the desired output, especially how multiple GPUs are
aggregated to increase the performance. They are the core contribution enabling
a flexible resource configuration.

\begin{description}
\item [Root compound] Define an empty top-level compound when
  synchronizing multiple destination views. Multiple destination views
  are used for multi-display systems, e.g., a PowerWall or CAVE. All
  windows used for one display surface should be swap-locked (see below)
  to provide a seamless image. A single destination view is typically
  used for providing scalability to a single workstation window.
\item [Destination compound(s)] Define one compound for each destination
  channel, either as a child of the empty group, or as a top-level compound. Set
  the destination channel by using the canvas, segment, layout and view name or
  index. The compound frustum will be calculated automatically based on the
  segment or view frustum. Note that one segment may created multiple
  view/segment channels, one for each view intersection of each layout used on
  the canvas. Only the compounds belonging to the active layout of a canvas are
  active at runtime.
\item[Scalability] If desired, define scalability for each of your
  destination compounds. Add one compound using a source channel for
  each contributor to the rendering. The destination channel may also be
  used as a source.
  \begin{description}
  \item[Decomposition] On each child compound, limit the rendering task of that
    child by setting the \textsf{viewport}, \textsf{range}, \textsf{period} and
    \textsf{phase}, \textsf{pixel}, \textsf{subpixel}, \textsf{eye} or
    \textsf{zoom} as desired.
  \item[Runtime Adjustments] A \textsf{load\_equalizer} may be used on
    the destination compounds to set the \textsf{viewport} or
    \textsf{range} of all children each frame, based on the current
    load. A \textsf{view\_equalizer} may be used on the root compound to
    assign resources to all destination compounds, which have to use
    \textsf{load\_equalizer}s. A \textsf{framerate\_equalizer} should be
    used to smoothen the framerate of DPlex compounds. A
    \textsf{DFR\_equalizer} may be used to set the zoom of a compound to
    achieve a constant framerate. One compound may have multiple
    equalizers, e.g., a \textsf{load\_equalizer} and a
    \textsf{DFR\_equalizer} for a 2D compound with a constant framerate.
  \item[Recomposition] For each source compound, define an
    \textsf{output\_frame} to read back the result. Use this output frame
    as an \textsf{input\_frame} on the destination compound. The frames
    are connected with each other by their name, which has to be unique
    within the root compound tree. For parallel compositing, describe
    your algorithm by defining multiple input and output frames across
    all source compounds.
  \end{description}
\end{description}

\subsubsection{Compound Channels}
Each compound has a channel, which is used by the compound to execute the
rendering tasks. One channel might be used by multiple compounds. Compounds are
only active if their corresponding destination channel is active, that is, if
the parent layout of the view which created the destination channel is active on
at least one canvas.

Unused channels, windows, pipes and nodes are not instantiated during
initialization.  Switching an active layout may cause rendering resources to be
stopped and started. The rendering tasks for the channels are computed by the
server and send to the appropriate render client nodes at the beginning of each
frame.

\subsubsection{\label{sFrustum}Frustum}

Compounds have a frustum description to define the physical layout of the
display environment. The frustum specification is described in
\sref{sCanvas}. The frustum description is inherited by the children, therefore
the frustum is defined on the topmost compound, typically by the corresponding
segment.

\subsubsection{Compound Classification}
The channels of the leaf compounds in the compound tree are designated
as source channels. The topmost channel in the tree is the destination
channel. One compound tree might have multiple destination channels,
e.g., for a swap-synchronized immersive installation.

All channels in a compound tree work for the destination channel. The
destination channel defines the 2D pixel viewport rendered by all leaf
compounds. The destination channel and pixel viewport cannot be
overridden by child compounds.

\subsubsection{Tasks}
Compounds execute a number of tasks: clear, draw, assemble and readback. By
default, a leaf compound executes all tasks and a non-leaf compound assemble and
readback. A non-leaf compound will never execute the draw task.

A compound can be configured to execute a specific set of tasks, for
example to configure the multiple steps used by binary-swap compositing.

\subsubsection{Decomposition - Attributes}
Compounds have attributes which configure the decomposition of the destination
channel's rendering, which is defined by the viewport, frustum and database. A
\textsf{viewport} decomposes the destination channel and frustum in screen
space. A \textsf{range} tells the application to render a part of its database,
and an \textsf{eye} rendering pass can selectively render different stereo
passes. A \textsf{pixel} parameter adjusts the frustum so that the source
channel renders an even subset of the parent's pixels. A \textsf{subpixel}
parameter tells the source channels to render different samples for one pixel to
perform anti-aliasing or depth-of-field rendering. Setting one or multiple
attributes causes the parent's view to be decomposed accordingly. Attributes are
cumulative, that is, intermediate compound attributes affect and therefore
decompose the rendering of all their children.

\subsubsection{Recomposition - Frames}
Compounds use output and input frames to configure the recomposition of
the resulting pixel data from the source channels. An output frame
connects to an input frame of the same name. The selected frame buffer
data is transported from the output channel to the input channel. The
assembly routine of the input channel will block on the availability of
the output frame. This composition process is extensively described in
\sref{sCompositing}. Frame names are only valid within the compound
tree, that is, an output frame from one compound tree cannot be used as
an input frame of another compound tree.

\section{Compositing}

\section{Load Balancing}

\section{Distribution Layer}


\chapter{Parallel Rendering Modes}


\chapter{Compositing Optimisations}


\chapter{Load Balancing Algorithms}


\chapter{Data Distribution and Synchronization}


\chapter{Conclusion}

\section{Future Work}


\if 0

\chapter{Problem Statement}

Visualization of large amounts of data has been always been encumbered by
sufficient system software. Compared to other domains, such as HPC simulations,
large data visualization has received relatively little attention in both
research and development. Consequently, there is a large amount of data which
has not been explored sufficiently. In particular in scientific visualization,
where data is often spatial and temporal, even simple visualizations can extract
new information and provide a valuable tool to domain scientists for discovery.

The central theme of the proposed research is therefore: {\bf How can we improve
the capabilities of existing visualization algorithms for rendering large
amounts of data?} This generic problem can be researched more concretely along
the following research questions:
\begin{compactenum}
\item How can we improve the rendering performance of visualization applications to enable users to explore more data?
    \begin{compactenum}
    \item What new algorithms will decrease the time needed to composite rendering results, in particular for sort-last rendering?
    \item How can we improve load-balancing for sort-first rendering, in particular for large display systems?
    \end{compactenum}
\item How can we reduce end-to-end system latency for better user experience?
    \begin{compactenum}
    \item In a generic parallel rendering framework, how can we schedule the different rendering stages to minimize the latency for the user?
    \item How can we architect the parallel rendering framework to minimize synchronization between threads?
    \end{compactenum}
\item How can we maximize the impact of this research on large data scientists?
\end{compactenum}

The obvious solution to the problem is to utilize more compute resources to
parallelize and scale the rendering algorithm. The goal can either be to
increase strong scaling (render a given data set faster) or weak scaling (render
a larger data set at roughly the same speed). To efficiently use more resources,
we need to research increasing the parallelism of existing algorithms, and to
reduce the bottleneck in the image compositing stage.

An important collateral problem is the overall latency of the rendering system,
that is, the time between a user input and its resulting output frame. While
this is lower-bounded by the framerate of the rendering, oftentimes algorithmic
or implementation choices increase the total system latency. Often this is a
side effect of improving the rendering performance, but it also decreases the
usability of the application for interactive usage. One typical example is
pipelining of operations in the rendering pipeline.

Visualizing large amounts of data often goes hand in hand with the usage of
high-resolution displays. Since large amounts of data tends to have a lot of
detail, high-resolution desktop screens (4K or 8K resolution), as well as
high-resolution display walls, help tremenduously in recognising and
understanding details of the data. The high resolution however aggravetes all
the aforementioned problems: The increased pixel count reduces rendering
performance, requires better compositing algorithms, and increases the latency
due to longer transfer times during compositing and display. Since pixel count
increases quadratically with display size, this problem will become more
important as display resolution increases.

\chapter{Proposed Solution} % 2-6

Parallel rendering has received a lot of attention in the last couple of
decades, yet libraries and frameworks to develop parallel rendering applications
are scarcely available. Consequently, there are only a few applications which
can utilize parallelism for rendering, let alone do so efficiently at scale.
This research proposes to address these shortcomings by developing {\bf reusable
software components} to make parallel rendering programs easier to develop, by
generalizing existing research into reusable software implementations.

Based on these foundations, we propose to research new algorithms to improve
rendering performance. In particular we see potential in improving the
scalability through better {\bf load balancing}, {\bf image compositing}
algorithms, and {\bf holistic optimization} of the whole rendering pipeline
under control of our framework. Orthogonally to this algorithmic research, we
propose to research {\bf data processing and data access} strategies for
parallel rendering applications. This is particularly important, yet
underdeveloped, in the context of the visual analysis for HPC simulation
results.

Previous parallel rendering approaches typically failed in one of the following
system requirements:
%
\begin{compactenum}
\item generic application support, instead of domain-specific solution
\item scalable abstraction of the graphics layer
\item exploit existing code infrastructure, such as proprietary scene graphs, molecular data structures, level-of-detail and geometry databases
\end{compactenum}

To date, generic and scalable parallel rendering frameworks that can be adopted
to a wide range of scientific visualization domains are not yet readily
available. Furthermore, flexible configurability to arbitrary cluster and
multi-display configurations has also not been addressed in the past, but is of
immense practical importance to scientists depending on high-performance
interactive visualization as a scientific tool. We propose a novel flexible
framework for parallel rendering that supports scalable performance,
configuration flexibility, is minimally invasive with respect to adapting
existing visualization applications, and is applicable to virtually any
scientific visualization application domain.

To that end, this work aims to significantly advance the system design and
implementation of flexible, distributed and cluster-parallel rendering
frameworks as well as algorithms and system design for large data processing in
the context of interactive visualization. The core of this proposal is the
Equalizer project, a foundation for scalable, multi-GPU visualization software
in all application domains. The main contributions of such a parallel rendering
system are:
%
\begin{compactenum}
\item novel concept for flexible runtime configuration of graphics system resources
\item easy specification of parallel task decomposition and image compositing algorithms
\item automatic decomposition and distributed execution of rendering tasks according to the configuration
\item support for polygonal and volume rendering for opaque and transparent geometries
\item fully decentralized software architecture providing network swap barrier synchronization and data distribution functionality
\item support for low-latency distributed frame synchronization and image compositing
\item minimally invasive programming model
\end{compactenum}

The broader impact of this work revolves around the development and improvement
of a generic, flexible and scalable parallel rendering infrastructure applicable
to a large number of application domains. The expected improvements of the
proposed activities in distributed parallel parallel rendering will be
integrated into open source software libraries and as such will be available to
the general public and especially to developers of high-performance
visualization and interactive rendering applications. This approach will
maximize the impact of this research on large data scientists (research question
4).

A strong focus during the development is to architect the framework for
scalability to address research questions 1 and 2. Based on my previous work and
the study of existing implementations, scalability in parallel rendering today
is mostly limited by excessive synchronization between execution threads,
imbalance in the task decomposition, and compositing performance. Equalizer only
has the following necessary synchronization points:
%
\begin{compactenum}
\item Swap synchronization between output channels to the same display system
\item Finalization of rendering frames given a configurable latency for all render threads
\item Availability of image data for compositing between the source and destination threads
\end{compactenum}
%
This architecture has proven to provide good scalability by inherently allowing
the pipelining of data synchronization, rendering and compositing tasks
(research question 1), while simultaneously minimizing the time to display
results by letting render threads execute as early as possible (research
question 2). Furthermore, this will provide better scalability for the more
specific research questions 1(a) and 1(b).

The following publications are planned or already published to address the corresponding research question:
\begin{compactenum}
    \item
    \begin{compactenum}
        \item In \cite{EP:07, MEP:10, EBAHMP:12} we presented different algorithms to optimize the compositing step for sort-last rendering and optimisations for modern multi-GPU NUMA nodes.
        \item In \cite{EEP:11, SPEP:16} we presented novel load-balancing algorithms for sort-first rendering.
    \end{compactenum}
    \item Our first system paper \cite{EMP:09} introduces the architecture of a parallel rendering framework with minimal synchronization and optimized task scheduling, including an in-depth experimental analysis. \cite{EBAHMP:12} introduced further scheduling optimizations.
    \item Our foundation systems paper \cite{EMP:09} and applications work \cite{HBBES:13} provide evidence on the sustainability of our approach. We submitted a follow-on systems paper to ACM Transactions on Visualization and Graphics \cite{ESP:18}. This paper also contains background on applications and integrations of Equalizer in other software packages.
\end{compactenum}

Based on this generic parallel rendering framework, we propose to research
concrete algorithms and applications. We propose an engineering-driven approach
which will analyse existing algorithms, improve them incrementally by focusing
on one aspect of a large data application, and then compare our new research
against existing work. Since our research questions are largely performance
related, this comparison will be in most cases performed through benchmarking.
In particular, we see potential in:
%
\begin{compactdesc}
\item [Load-balancing for rendering resources:] While basic algorithms have been
proposed for reactive and proactive load-balancing of simple rendering tasks,
research is still needed for improving the resource utilisation for large-scale
parallelization, as well as for rendering in more complex multi-display
environments such as tiled display walls and immersive installations. The
results of this research is directly measurable through application benchmarking
of representative data sets. We consider this research goal achieved if we
proposed and implemented new algorithms which can consistently deliver better
performance over existing work, addressing research question 1(b).
\item [Compositing of the rendering results:] Previous research has focused on
the scalability for very large scale HPC runs in the order of hundreds of
thousands of cores, which are oftentimes not interactive by nature. We propose
to improve image compositing performance for interactive applications on
medium-sized (up to hundreds of GPUs) visualization clusters through analysing
and optimising image compositing algorithms. As with load-balancing, this area
of research can be considered achieved if benchmarks show consistent improvement
over state of the art algorithms, addressing research question 1(a).
\item [Applications for parallel rendering:] While a few parallel rendering
applications exist, developing them is still a significant undertaking. We
propose to extend existing rendering applications and algorithms for scientific
visualization for parallel rendering. Not only will this create new results and
capabilites for large data visualization, it also improves the general
applicability and ease of use of our generic parallel rendering components. We
consider this goal achieved if our framework is used in multiple visualization
applications. A side effect of this goal is addressing research question 4.
\item [Data management for visualization of HPC data:] We see a substantial
potential in combining big data management strategies from the cloud computing
domain to processing and visualising HPC simulation data. Paradoxically, storage
systems for HPC are often optimised for large, sequential access, which is
predominant during write, but not typical for analysis and visualization which
use more, but smaller scattered read accesses. This is an exploratory research
goal, where we hope to demonstrate the usefulness of cloud computing storage
systems for HPC storage and linked large data visualizations. We consider this
goal reached if we evaluated multiple approaches to data storage against their
traditional parallel filesystem implementation, and can make recommendations for
future research. The evaluation will again be benchmark-based, by measuring the
time to solution for typical data access patterns.
\end{compactdesc}

This research will have a sustainable impact on how we use large-scale
visualization systems as their commoditization makes them affordable to many
more organizations. This is due to the research approach of using an
incremental, engineering-driven and data-validated strategy, open source
implementation of most software artefacts, the development of high-quality
foundations, as well as the collaboration with both research and industry
partners during the research.

The proposed research will have a direct, significant impact on accelerating the
simulation-based research performed in the Blue Brain Project. On one hand, the
data distribution capabilities will allow faster development of scalable,
neuroscience-specific visualization applications for the BBP. This is of
particular importance as we foresee the need to visualize different modalities
at different brain scales as the simulations grow in complexity and data size in
the future.

\chapter{Research Plan\label{sPlan}}

The proposed research is heavily based on prior software engineering work in the
domain, and will leverage a strong parallel rendering system to enable novel
research within a non-trivial software stack. Consequently, a large portion of
the plan is the development of the parallel rendering framework, where the
timeline has been reverse-engineered from the work performed leading to this
proposal.

The integration of these frameworks into applications is not part of this
research schedule, since it is a non-research activity. These developments have
been and will be funded by other means. This work is nevertheless an important
part of this project, as it validates the general applicability of this research
in academia and industry.
\clearpage
The research is structured as follows:
%
\begin{compactdesc}
\item[M1-3: System Architecture:] Outline the general system architecture,
configuration structure and entities, class hierarchy and API. Research
third-party technologies to be used in the implementation.
\item[M4-10: Distributed Execution Layer:] First iteration of the implementation
of the distributed execution layer allowing dynamic configurations and
communication patterns.
\item[M10-16: Multi-Display Parallel Rendering Framework:] Based on the
distributed execution layer, develop a first parallel rendering framework
capable of driving multi-display environments for monoscopic rendering,
including the automatic launch of the rendering processes from the main
application.
\item[M17: Stereoscopic Rendering:] Implement stereoscopic rendering using
configurable interocular distance.
\item[M18: Immersive Rendering:] Head-tracking API and configuration entities,
calculation of corresponding off-axis frusta.
\item[M19: Scalable Rendering Architecture:] Design configuration and class
hierarchy for scalable rendering modes, including task decomposition and
parallel compositing algorithms.
\item[M20-24: Basic Scalable Rendering:] Implement basic decompositions (2D, DB,
Eye) and corresponding parallel compositing algorithms (2D, direct-send,
binary-swap).
\item[M25-30: Advanced Scalable Rendering:] Implement advanced compounds (Pixel,
DPlex) and compositing optimizations (realtime image compression, region of
interest, etc.).
\item[M30-32: First Publication:] Benchmarking and systems paper.
\item[M33-36: Load Balancing:] Implement load-balancing for multi-display
setups. improved automatic load-balancing using region of interest.
\item[M37-38: Second Publication:] Benchmarking and load-balancing paper.
\item[M39-42: Compositing Research:] Research compositing optimizations.
\item[M43-44: Third Publication:] Benchmarking and compositing paper.
\item[PhD Proposal]
\item[M45-46: Fourth Publication:] Benchmarking and updated systems paper.
\item[M47-52: Dissertation:] Write and defend dissertation.
\end{compactdesc}

\fi
